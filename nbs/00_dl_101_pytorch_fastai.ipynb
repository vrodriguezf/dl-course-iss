{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning 101 with Pytorch and fastai\n",
    "\n",
    "> Some code and text snippets have been extracted from the book [\\\"Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD\\\"](https://course.fast.ai/), and from these blog posts [[ref1](https://muellerzr.github.io/fastblog/2021/02/14/Pytorchtofastai.html)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dataset that contains $(x,y)$ pairs sampled from the linear function $y = ax + b+ \\epsilon$. To do this, we'll create a PyTorch's `TensorDataset`.\n",
    "\n",
    "A PyTorch tensor is nearly the same thing as a NumPy array. The vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster (given lots of values to work on). In addition, PyTorch can automatically calculate derivatives of these operations, including combinations of operations. These two things are critical for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(plt.Axes.scatter)\n",
    "def show_TensorFunction1D(x, y, y_hat=None, return_fig=False, **kwargs):\n",
    "    r\"\"\"\n",
    "        Displays the 1D function `y`(`x`), with optional predictions `y_hat`\n",
    "    \"\"\"\n",
    "    figure = plt.figure()\n",
    "    ax = figure.add_subplot(111)\n",
    "    ax.scatter(x, y, **kwargs)\n",
    "    if y_hat is not None: ax.scatter(x, y_hat, **kwargs)\n",
    "    if return_fig: return figure\n",
    "\n",
    "\n",
    "def linear_function_dataset(a, b, n=100, show_plot=False):\n",
    "    r\"\"\"\n",
    "        Creates a Pytorch's `TensorDataset` with `n` random samples of the \n",
    "        linear function y = `a`*x + `b`. `show_plot` dcides whether or not to \n",
    "        plot the dataset\n",
    "    \"\"\"\n",
    "    x = torch.randn(n, 1)\n",
    "    y = a*x + b + 0.1*torch.randn(n, 1)\n",
    "    if show_plot:\n",
    "        show_TensorFunction1D(x, y, marker='.')\n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 3\n",
    "n = 100\n",
    "data = linear_function_dataset(a, b, n, show_plot=True)\n",
    "test_eq(type(data), TensorDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every machine/deep learning experiment, we need to have at least two datasets:\n",
    " - training: used to train the model\n",
    " - validation: used to validate the model after each training step. It allows to detect overfitting and adjust the hyperparameters of the model properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = linear_function_dataset(a, b, n=100, show_plot=True)\n",
    "valid_ds = linear_function_dataset(a, b, n=20, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataloader combines a dataset an a sampler that samples data into **batches**, and provides an iterable over the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dl, 1):\n",
    "    x, y = data\n",
    "    print(f'batch {i}: x={x.shape} ({x.device}), y={y.shape} ({y.device})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a linear regression model in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `torch.nn.Module` is the base structure for all models in Pytorch. It mostly helps to register all the trainable parameters. A module is an object of a class that inherits from the PyTorch `nn.Module` class.\n",
    "\n",
    "To implement an `nn.Module` you just need to:\n",
    "\n",
    "- Make sure the superclass __init__ is called first when you initialize it.\n",
    "- Define any parameters of the model as attributes with nn.Parameter. To tell `Module` that we want to treat a tensor as a parameter, we have to wrap it in the `nn.Parameter` class. All PyTorch modules use `nn.Parameter` for any trainable parameters. This class doesn't actually add any functionality (other than automatically calling `requires_grad`). It's only used as a \"marker\" to show what to include in parameters:\n",
    "- Define a forward function that returns the output of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinRegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.randn(1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x): return self.a*x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinRegModel()\n",
    "pa, pb = model.parameters()\n",
    "pa, pa.shape, pb, pb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1)\n",
    "out = model(x)\n",
    "x, x.shape, out, out.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the thing the machine is using as the measure of performance to decide how to update model parameters. The loss function is simple enough for a regression problem, we'll just use the Mean Square Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "loss_func(x, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data, a model, and a loss function; we only need one more thing we can fit a model, and that's an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need to push our model and our batches to the GPU. Calling `cuda()` on a model or a tensor this class puts all these parameters on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we will need to compute all the gradients of a given loss with respect to its parameters, which is known as the *backward pass*. The *forward pass* is where we compute the output of the model on a given input, based on the matrix products. PyTorch computes all the gradients we need with a magic call to `loss.backward`. The backward pass is the chain rule applied multiple times, computing the gradients from the output of our model and going back, one layer at a time. \n",
    "\n",
    "In Pytorch, Each basic function we need to differentiate is written as a `torch.autograd.Function` object that has a `forward` and a `backward` method. PyTorch will then keep trace of any computation we do to be able to properly run the backward pass, unless we set the `requires_grad` attribute of our tensors to `False`. \n",
    "\n",
    "For minibatch gradient descent (the usual way of training in deep learning), we calculate gradients on batches. Before moving onto the next batch, we modify our model's parameters based on the gradients. For each iteration through our dataset (which would be called an **epoch**), the optimizer would perform as many updates as we have batches.\n",
    "\n",
    "There are two important methods in a Pytorch optimizer:\n",
    "- `zero_grad`: In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. `zero_grad` just loops through the parameters of the model and sets the gradients to zero. It also calls `detach_`, which removes any history of gradient computation, since it won't be needed after `zero_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train(model, device, train_dl, loss_func, opt_func, epoch_idx):\n",
    "    r\"\"\"\n",
    "        Train `model` for one epoch, whose index is given in `epoch_idx`. The \n",
    "        training loop will iterate through all the batches of `train_dl`, using\n",
    "        the the loss function given in `loss_func` and the optimizer given in `opt_func`\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    batches_processed = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_dl, 1):\n",
    "        x, y = x.to(device), y.to(device) # Push data to GPU\n",
    "\n",
    "        opt_func.zero_grad() # Reset gradients\n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "        loss = loss_func(output, y)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimizer step\n",
    "        opt_func.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        batches_processed += 1\n",
    "    print(f'Train loss [Epoch {epoch_idx}]: {running_loss/batches_processed : .2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    train(model, device, train_dl, loss_func, opt_func, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the parameters of the regression model are getting closer to the truth values `a` and `b` from the linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the model requires only a forward pass, it's just inference. Disabling gradient calculation with the method `torch.no_grad()` is useful for inference, when you are sure that you will not call :meth:`Tensor.backward()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, dl):\n",
    "    running_loss = 0.\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = loss_func(output, y)\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "    print(f'Valid loss: {running_loss/total_batches : .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, device, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to spot overfitting, it is useful to validate the model after each training epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs +1):\n",
    "    train(model, device, train_dl, loss_func, opt_func, epoch)\n",
    "    validate(model, device, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracting the manual training loop: moving from Pytorch to fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.callback.progress import ProgressCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can entirely replace the custom training loop with fastai's. That means you can get rid of `train()`, `validate()`, and the epoch loop in the original code, and replace it all with a couple of lines.\n",
    "\n",
    "fastai's training loop lives in a `Learner`. The Learner is the glue that merges everything together (Datasets, Dataloaders, model and optimizer) and enables to train by just calling a `fit` function.\n",
    "\n",
    "fastai's `Learner` expects DataLoaders to be used, rather than simply one DataLoader, so let's make that. We could just do `dls = Dataloaders(train_dl, valid_dl)`, to keep the PyTorch Dataloaders. However, by using a fastai `DataLoader` instead, created directly from the `TensorDataset` objects, we have some automations, such as automatic pushing of the data to GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model=LinRegModel(), loss_func=nn.MSELoss(), opt_func=SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything needed to do a basic `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Learner allows us to easily gather the model predictions for the validation set, which we can use for visualisation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, preds, outputs = learn.get_preds(with_input=True)\n",
    "inputs.shape, preds.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_TensorFunction1D(inputs, outputs, y_hat=preds, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example, we will create the dataset by sampling values from a non linear sample $y(x) = -\\frac{1}{100}x^7 - x^4 - 2x^2 - 4x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def nonlinear_function_dataset(n=100, show_plot=False):\n",
    "    r\"\"\"\n",
    "        Creates a Pytorch's `TensorDataset` with `n` random samples of the \n",
    "        nonlinear function y = (-1/100)*x**7 -x**4 -2*x**2 -4*x + 1 with a bit \n",
    "        of noise. `show_plot` decides whether or not to plot the dataset\n",
    "    \"\"\"\n",
    "    x = torch.rand(n, 1)*20 - 10 # Random values between [-10 and 10]\n",
    "    y = (-1/100)*x**7 -x**4 -2*x**2 -4*x + 1 + 0.1*torch.randn(n, 1)\n",
    "    if show_plot:\n",
    "        show_TensorFunction1D(x, y, marker='.')\n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "ds = nonlinear_function_dataset(n, show_plot=True)\n",
    "x, y = ds.tensors\n",
    "test_eq(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the trainin and test dataset, and build the Dataloaders with them, this time directly in fastai, using the `Dataloaders.from_dsets` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = nonlinear_function_dataset(n=1000)\n",
    "valid_ds = nonlinear_function_dataset(n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization in deep learning are used to make optimization easier by smoothing the loss surface of the network. We will normalize the data based on the mean and std of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = train_ds.tensors[1].mean()\n",
    "norm_std = train_ds.tensors[1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_norm = TensorDataset(train_ds.tensors[0],\n",
    "                              (train_ds.tensors[1] - norm_mean)/norm_std)\n",
    "valid_ds_norm = TensorDataset(valid_ds.tensors[0],\n",
    "                              (valid_ds.tensors[1] - norm_mean)/norm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(train_ds_norm, valid_ds_norm, bs = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Multi Layer Perceptron with 3 hidden layers. These networks are also known as Feed-Forward Neural Networks. The layers aof this type of networks are known as Fully Connected Layers, because, between every subsequent pair of layers, all the neurons are connected to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Neural network architecture\" caption=\"Neural network\" src=\"https://i.imgur.com/5ZWPtRS.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way of wrapping several layers in Pytorch is using the `nn.Sequential` module. It creates a module with a `forward` method that will call each of the listed layers or functions in turn, without us having to do the loop manually in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP3(nn.Module):\n",
    "    r\"\"\"\n",
    "        Multilayer perceptron with 3 hidden layers, with sizes `nh1`, `nh2` and\n",
    "        `nh3` respectively.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in=1, nh1=200, nh2=100, nh3=50, n_out=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_in, nh1),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(nh1, nh2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nh2, nh3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nh3, n_out)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dls.one_batch()\n",
    "model = MLP3()\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, MLP3(), loss_func=nn.MSELoss(), opt_func=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, preds, outputs = learn.get_preds(with_input = True)\n",
    "show_TensorFunction1D(inputs, outputs, y_hat=preds, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these results with the ones by our previous linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lin = Learner(dls, LinRegModel(), loss_func=nn.MSELoss(), opt_func=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lin.fit(20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, preds, outputs = learn_lin.get_preds(with_input = True)\n",
    "show_TensorFunction1D(inputs, outputs, y_hat=preds, marker='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
